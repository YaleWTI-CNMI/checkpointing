
CondaError: Run 'conda init' before 'conda deactivate'

/home/pl543/workspace/conda/torch_2.4/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
slurmstepd: error: *** STEP 63821.0 ON r4518u09n01 CANCELLED AT 2024-10-30T10:25:43 ***
Step: 1000; Loss: 0.0079
Step: 2000; Loss: 0.0078
Step: 3000; Loss: 0.0057
Step: 4000; Loss: 0.0129
Step: 5000; Loss: 0.0131
Step: 6000; Loss: 0.0097
Step: 7000; Loss: 0.0109
Step: 8000; Loss: 0.0076
Step: 9000; Loss: 0.0102
Step: 10000; Loss: 0.0095
Step: 11000; Loss: 0.0085
Step: 12000; Loss: 0.0059
Step: 13000; Loss: 0.0052
Step: 14000; Loss: 0.0069
Step: 15000; Loss: 0.0106
Step: 16000; Loss: 0.0076
Step: 17000; Loss: 0.0107
Step: 18000; Loss: 0.0136
Step: 19000; Loss: 0.0095
Step: 20000; Loss: 0.0073
Step: 21000; Loss: 0.0115
Step: 22000; Loss: 0.0067
Step: 23000; Loss: 0.0073
Step: 24000; Loss: 0.0066
Step: 25000; Loss: 0.0055
Step: 26000; Loss: 0.0084
Step: 27000; Loss: 0.0091
Step: 28000; Loss: 0.0071
Step: 29000; Loss: 0.0055
Step: 30000; Loss: 0.0092
Step: 31000; Loss: 0.0124
Step: 32000; Loss: 0.0056
Step: 33000; Loss: 0.0102
Step: 34000; Loss: 0.0118
Step: 35000; Loss: 0.0110
Step: 36000; Loss: 0.0104
Step: 37000; Loss: 0.0088
Step: 38000; Loss: 0.0130
Step: 39000; Loss: 0.0075
Step: 40000; Loss: 0.0097
Step: 41000; Loss: 0.0100
Step: 42000; Loss: 0.0095
Step: 43000; Loss: 0.0100
Step: 44000; Loss: 0.0095
Step: 45000; Loss: 0.0094
Step: 46000; Loss: 0.0087
Step: 47000; Loss: 0.0074
Step: 48000; Loss: 0.0069
Step: 49000; Loss: 0.0138
Step: 50000; Loss: 0.0136
Step: 51000; Loss: 0.0093
Step: 52000; Loss: 0.0129
Step: 53000; Loss: 0.0094
Step: 54000; Loss: 0.0116
Step: 55000; Loss: 0.0108
Step: 56000; Loss: 0.0109
Step: 57000; Loss: 0.0100
Step: 58000; Loss: 0.0067
Step: 59000; Loss: 0.0086
Step: 60000; Loss: 0.0139
Step: 61000; Loss: 0.0092
Step: 62000; Loss: 0.0085
Step: 63000; Loss: 0.0108
Step: 64000; Loss: 0.0077
Step: 65000; Loss: 0.0187
Step: 66000; Loss: 0.0140
Step: 67000; Loss: 0.0132
Step: 68000; Loss: 0.0065
Step: 69000; Loss: 0.0090
Step: 70000; Loss: 0.0117
Step: 71000; Loss: 0.0077
Step: 72000; Loss: 0.0147
Step: 73000; Loss: 0.0091
Step: 74000; Loss: 0.0066
Step: 75000; Loss: 0.0108
Step: 76000; Loss: 0.0088
Step: 77000; Loss: 0.0101
Step: 78000; Loss: 0.0076
Step: 79000; Loss: 0.0101
Step: 80000; Loss: 0.0067
Step: 81000; Loss: 0.0109
Step: 82000; Loss: 0.0134
Step: 83000; Loss: 0.0076
Step: 84000; Loss: 0.0080
Step: 85000; Loss: 0.0112
Step: 86000; Loss: 0.0089
Step: 87000; Loss: 0.0118
Step: 88000; Loss: 0.0102
Step: 89000; Loss: 0.0106
Step: 90000; Loss: 0.0091
Step: 91000; Loss: 0.0120
Step: 92000; Loss: 0.0080
Step: 93000; Loss: 0.0091
Step: 94000; Loss: 0.0089
Step: 95000; Loss: 0.0072
Step: 96000; Loss: 0.0121
Step: 97000; Loss: 0.0090
Step: 98000; Loss: 0.0132
Step: 99000; Loss: 0.0099
Step: 100000; Loss: 0.0073
Step: 101000; Loss: 0.0112
Step: 102000; Loss: 0.0107
Step: 103000; Loss: 0.0144
Step: 104000; Loss: 0.0112
Step: 105000; Loss: 0.0111
Step: 106000; Loss: 0.0117
Step: 107000; Loss: 0.0081
Step: 108000; Loss: 0.0121
Step: 109000; Loss: 0.0122
Step: 110000; Loss: 0.0142
Step: 111000; Loss: 0.0088
Step: 112000; Loss: 0.0087
Step: 113000; Loss: 0.0135
Step: 114000; Loss: 0.0083
Step: 115000; Loss: 0.0143
Step: 116000; Loss: 0.0126
Step: 117000; Loss: 0.0107
Step: 118000; Loss: 0.0110
Step: 119000; Loss: 0.0073
Step: 120000; Loss: 0.0074
Step: 121000; Loss: 0.0061
Step: 122000; Loss: 0.0088
Step: 123000; Loss: 0.0079
Step: 124000; Loss: 0.0103
Step: 125000; Loss: 0.0086
Step: 126000; Loss: 0.0069
Step: 127000; Loss: 0.0099
Step: 128000; Loss: 0.0109
Step: 129000; Loss: 0.0095
Step: 130000; Loss: 0.0104
Step: 131000; Loss: 0.0063
Step: 132000; Loss: 0.0078
Step: 133000; Loss: 0.0106
Step: 134000; Loss: 0.0079
Step: 135000; Loss: 0.0102
Step: 136000; Loss: 0.0134
Step: 137000; Loss: 0.0168
Step: 138000; Loss: 0.0102
Step: 139000; Loss: 0.0096
Step: 140000; Loss: 0.0064
Step: 141000; Loss: 0.0085
Step: 142000; Loss: 0.0076
Step: 143000; Loss: 0.0100
Step: 144000; Loss: 0.0082
Step: 145000; Loss: 0.0113
Step: 146000; Loss: 0.0119
Step: 147000; Loss: 0.0105
Step: 148000; Loss: 0.0086
Step: 149000; Loss: 0.0055
Step: 150000; Loss: 0.0076
Step: 151000; Loss: 0.0055
Step: 152000; Loss: 0.0078
Step: 153000; Loss: 0.0122
Step: 154000; Loss: 0.0088
Step: 155000; Loss: 0.0117
Step: 156000; Loss: 0.0120
Step: 157000; Loss: 0.0112
Step: 158000; Loss: 0.0124
Step: 159000; Loss: 0.0075
Step: 160000; Loss: 0.0134
Step: 161000; Loss: 0.0108
Step: 162000; Loss: 0.0065
Step: 163000; Loss: 0.0112
Step: 164000; Loss: 0.0105
Step: 165000; Loss: 0.0159
Step: 166000; Loss: 0.0090
Step: 167000; Loss: 0.0054
Step: 168000; Loss: 0.0108
Step: 169000; Loss: 0.0127
Step: 170000; Loss: 0.0147
Step: 171000; Loss: 0.0108
Step: 172000; Loss: 0.0126
Step: 173000; Loss: 0.0080
Step: 174000; Loss: 0.0113
Step: 175000; Loss: 0.0046
Step: 176000; Loss: 0.0077
Step: 177000; Loss: 0.0102
Step: 178000; Loss: 0.0083
Step: 179000; Loss: 0.0048
Step: 180000; Loss: 0.0101
Step: 181000; Loss: 0.0163
Step: 182000; Loss: 0.0081
Step: 183000; Loss: 0.0108
Step: 184000; Loss: 0.0089
Step: 185000; Loss: 0.0102
Step: 186000; Loss: 0.0096
Step: 187000; Loss: 0.0132
Step: 188000; Loss: 0.0102
Step: 189000; Loss: 0.0111
Step: 190000; Loss: 0.0120
Step: 191000; Loss: 0.0100
Step: 192000; Loss: 0.0116
Step: 193000; Loss: 0.0072
Step: 194000; Loss: 0.0072
Step: 195000; Loss: 0.0076
Step: 196000; Loss: 0.0142
Step: 197000; Loss: 0.0127
Step: 198000; Loss: 0.0117
Step: 199000; Loss: 0.0069
Step: 200000; Loss: 0.0112
Step: 201000; Loss: 0.0087
Step: 202000; Loss: 0.0105
Step: 203000; Loss: 0.0169
Step: 204000; Loss: 0.0081
Step: 205000; Loss: 0.0111
Step: 206000; Loss: 0.0084
Step: 207000; Loss: 0.0085
Step: 208000; Loss: 0.0118
Step: 209000; Loss: 0.0085
Step: 210000; Loss: 0.0113
Step: 211000; Loss: 0.0154
Step: 212000; Loss: 0.0117
Step: 213000; Loss: 0.0087
Step: 214000; Loss: 0.0084
Step: 215000; Loss: 0.0109
Step: 216000; Loss: 0.0082
Step: 217000; Loss: 0.0063
Step: 218000; Loss: 0.0119
Step: 219000; Loss: 0.0069
Step: 220000; Loss: 0.0092
Step: 221000; Loss: 0.0079
Step: 222000; Loss: 0.0108
Step: 223000; Loss: 0.0127
Step: 224000; Loss: 0.0083
Step: 225000; Loss: 0.0145
Step: 226000; Loss: 0.0089
Step: 227000; Loss: 0.0087
Step: 228000; Loss: 0.0098
Step: 229000; Loss: 0.0073
Step: 230000; Loss: 0.0132
Step: 231000; Loss: 0.0105
Step: 232000; Loss: 0.0100
Step: 233000; Loss: 0.0060
Step: 234000; Loss: 0.0088
Step: 235000; Loss: 0.0137
Step: 236000; Loss: 0.0106
Step: 237000; Loss: 0.0123
Step: 238000; Loss: 0.0114
Step: 239000; Loss: 0.0089
Step: 240000; Loss: 0.0108
Step: 241000; Loss: 0.0104
Step: 242000; Loss: 0.0112
Step: 243000; Loss: 0.0137
Step: 244000; Loss: 0.0074
Step: 245000; Loss: 0.0117
Step: 246000; Loss: 0.0096
Step: 247000; Loss: 0.0155
Step: 248000; Loss: 0.0099
Step: 249000; Loss: 0.0140
Step: 250000; Loss: 0.0081
Step: 251000; Loss: 0.0086
Step: 252000; Loss: 0.0086
Step: 253000; Loss: 0.0097
Step: 254000; Loss: 0.0103
Step: 255000; Loss: 0.0117
Step: 256000; Loss: 0.0085
Step: 257000; Loss: 0.0107
Step: 258000; Loss: 0.0113
Step: 259000; Loss: 0.0100
Step: 260000; Loss: 0.0195
Step: 261000; Loss: 0.0125
Step: 262000; Loss: 0.0071
Step: 263000; Loss: 0.0066
Step: 264000; Loss: 0.0139

====================================================================================================
detected preemption flag inside training loop
exiting gracefully (saving model checkpoint, etc.) ...
exiting now
====================================================================================================
Done
